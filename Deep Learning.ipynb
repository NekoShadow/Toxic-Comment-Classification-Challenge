{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Specify the embedding file\n",
    "EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "\n",
    "#Read the training and testing sets\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtain the text and labels for training\n",
    "X_train = train[\"comment_text\"].str.lower()\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "\n",
    "#Obtain the text for test data. Note that no label is given in the testing dataset\n",
    "X_test = test[\"comment_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the maximum number of features, maximum length of comments, and embedding size.\n",
    "max_features=100000\n",
    "maxlen=150\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a class for model evaluation\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize the corpus constructed from both the training text and testing text.\n",
    "tok=text.Tokenizer(num_words=max_features,lower=True)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "#Convert texts into a sequence of words\n",
    "X_train=tok.texts_to_sequences(X_train)\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "#Pads each sequence to the same length, i.e. 150\n",
    "x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the complete embedding matrix from the embedding file, each word is associated with a 300-dim vector.\n",
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tok.word_index\n",
    "#Prepare embedding matrix for this dataset\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:#Ignore the words that has an index out of range\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jianf\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Define a function to use RandomSearchCV for Keras models to tune parameters\n",
    "def create_model(learn_rate = 0.01, momentum = 0, init_mode='uniform', dropout_rate=0.1, weight_constraint=0):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,dropout=dropout_rate,recurrent_dropout=dropout_rate))(x)\n",
    "    x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = init_mode, kernel_constraint=maxnorm(weight_constraint))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool]) \n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "    ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "    callbacks_list = [ra_val,checkpoint, early]\n",
    "    #Compile model\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=learn_rate, momentum = momentum, decay=1e-6),metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "seed = 666\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "\n",
    "#Create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "#Define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "weight_constraint = [0, 1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.1, 0.2, 0.3]\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum, init_mode=init_mode, weight_constraint=weight_constraint, dropout_rate=dropout_rate)\n",
    "\n",
    "rand = RandomizedSearchCV(estimator=model, param_grid=param_grid, n_iter = 10, cv = 3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/4\n",
      "143488/143613 [============================>.] - ETA: 2s - loss: 0.0562 - acc: 0.9800\n",
      " ROC-AUC - epoch: 1 - score: 0.986515\n",
      "Epoch 00001: val_acc improved from -inf to 0.98279, saving model to best.hdf5\n",
      "143613/143613 [==============================] - 3051s 21ms/step - loss: 0.0562 - acc: 0.9801 - val_loss: 0.0446 - val_acc: 0.9828\n",
      "Epoch 2/4\n",
      "143488/143613 [============================>.] - ETA: 2s - loss: 0.0438 - acc: 0.9832\n",
      " ROC-AUC - epoch: 2 - score: 0.988919\n",
      "Epoch 00002: val_acc improved from 0.98279 to 0.98395, saving model to best.hdf5\n",
      "143613/143613 [==============================] - 3032s 21ms/step - loss: 0.0438 - acc: 0.9832 - val_loss: 0.0416 - val_acc: 0.9839\n",
      "Epoch 3/4\n",
      "143488/143613 [============================>.] - ETA: 2s - loss: 0.0410 - acc: 0.9841\n",
      " ROC-AUC - epoch: 3 - score: 0.988741\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 2708s 19ms/step - loss: 0.0410 - acc: 0.9841 - val_loss: 0.0423 - val_acc: 0.9836\n",
      "Epoch 4/4\n",
      "143488/143613 [============================>.] - ETA: 2s - loss: 0.0386 - acc: 0.9850\n",
      " ROC-AUC - epoch: 4 - score: 0.988571\n",
      "Epoch 00004: val_acc did not improve\n",
      "143613/143613 [==============================] - 2645s 18ms/step - loss: 0.0386 - acc: 0.9850 - val_loss: 0.0426 - val_acc: 0.9835\n",
      "Predicting....\n",
      "153164/153164 [==============================] - 897s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "rand.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)\n",
    "#Training themodel takes a lot of time, so I have saved the weights then load the model weights to get the results.\n",
    "#The model weights can be downloaded via: https://drive.google.com/open?id=1EACiAZMv1PcQKDUwEKoWLAaMPKulrGcL\n",
    "filepath=\"Weights.hdf5\"\n",
    "rand.load_weights(filepath)\n",
    "print('Predicting....')\n",
    "y_pred = rand.predict(x_test,batch_size=1024,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the predicting results\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
